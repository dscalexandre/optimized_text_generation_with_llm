# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTxo77WUJwjPYF8lyza1HgabcdsZJs25

## <font color='limegreen'>GENERATIVE AI AND NATURAL LANGUAGE PROCESSING</font>
### <font color='limegreen'>LLM MISTRAL-7B INSTRUCT - OPTIMIZED TEXT GENERATION</font>

### Step 1: Install Packages
"""

!pip install -r requirements.txt

!pip install -q -U watermark

"""### Step 2: Import Libraries"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

"""### Step 3: Package Versions"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext watermark
# %watermark --iversions

# Commented out IPython magic to ensure Python compatibility.
# %watermark -v -m

"""### Step 4: Check CUDA Environment"""

# CUDA (GPU) Environment Check and Configuration
import torch

if torch.cuda.is_available():
    print(f"CUDA device available: {torch.cuda.get_device_name(0)}")
    print(f"Number of available GPUs: {torch.cuda.device_count()}")
    print(f"Current GPU: {torch.cuda.current_device()}")
else:
    print("CUDA is not available. Please ensure you have a GPU environment configured in Colab.")

"""### Step 5: Load Model and Tokenizer"""

# Set the model identifier
checkpoint = "mistralai/Mistral-7B-Instruct-v0.3"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side="left")

# Load the pre-trained model
model = AutoModelForCausalLM.from_pretrained(checkpoint)

# Set the prompt
sentence_text = "Como funciona uma sessão psicológica?"
sentence = f"<s>[INST] {sentence_text} [/INST]"

# Tokenize the prompt
input_ids = tokenizer(sentence, return_tensors="pt")

"""### Step 6: Generate the model response"""

# Text Generation (Inference)
generated_ids = model.generate(
    **input_ids,
    max_new_tokens=540,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.95)

# Decoder the ids generated
generated_text_full = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# Extract the response from the model
try:
    response_content = generated_text_full.split("[/INST]")[-1].strip()
    normalized_sentence = sentence_text.lower().rstrip("?.! ").strip()

    if response_content.lower().startswith(normalized_sentence):
        generated_response = response_content[len(normalized_sentence):].lstrip("?. ").strip()
    else:
        generated_response = response_content

except Exception as e:
    print(f"Erro ao extrair resposta: {e}. Exibindo o texto gerado completo.")
    generated_response = generated_text_full.strip()

print("\n--- Texto Gerado ---")
print(sentence_text)
print(generated_response)

"""### End"""